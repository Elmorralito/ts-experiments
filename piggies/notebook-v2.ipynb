{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcb3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# # raw_data = pd.read_excel(\n",
    "# #     \"/Users/drest/Downloads/Book2.xlsx\",\n",
    "# #     sheet_name=\"Country_Avg\",\n",
    "# # )\n",
    "\n",
    "# # # Unpivot the DataFrame so each year/month is a row, not a column\n",
    "# # raw_data = raw_data.melt(id_vars=[\"Unnamed: 0\"], var_name=\"year\", value_name=\"value\")\n",
    "# # raw_data = raw_data.rename(columns={\"Unnamed: 0\": \"month\"})\n",
    "\n",
    "# # raw_data[\"eom\"] = pd.to_datetime(raw_data[\"year\"].astype(str) + \"-\" + raw_data[\"month\"].astype(str).str.strip() + \"-01\", format=\"%Y-%b-%d\") + MonthEnd(0)\n",
    "# # raw_data.drop(columns=[\"year\", \"month\"], inplace=True)\n",
    "# # weekly_data = pd.read_parquet(\".data/extracted/porkcolombia.parquet.snappy\")\n",
    "# # weekly_data = weekly_data.loc[weekly_data[\"ts\"] >= pd.to_datetime(\"2023-12-31\")]\n",
    "# # weekly_data[\"eom\"] = weekly_data[\"ts\"] + MonthEnd(0)\n",
    "# # weekly_data_ = weekly_data.groupby(\"eom\").agg({\"average\": \"mean\"}).reset_index().sort_values(\"eom\", ascending=False)\n",
    "# # combined = raw_data.merge(weekly_data_, on=\"eom\", how=\"left\")\n",
    "# # combined[\"price\"] = combined[[\"value\", \"average\"]].mean(axis=1, skipna=True)\n",
    "# # combined.drop(columns=[\"value\", \"average\"], inplace=True)\n",
    "# # combined.sort_values(\"eom\", ascending=False, inplace=True)\n",
    "# # combined.reset_index(drop=True).to_parquet(\".data/extracted/porkcolombia_country_avg.parquet.snappy\", index=False)\n",
    "# country_avg = pd.read_parquet(\".data/extracted/porkcolombia_country_avg.parquet.snappy\")\n",
    "# regional_data = pd.read_excel(\n",
    "#     \"/Users/drest/Downloads/Book2.xlsx\",\n",
    "#     sheet_name=\"Regional\",\n",
    "# )\n",
    "# regional_data.fillna(method=\"ffill\", inplace=True)\n",
    "# antioquia = regional_data[[\"Mes\", \"Antioquia\"]].rename(columns={\"Mes\": \"eom\", \"Antioquia\": \"price\"})\n",
    "# antioquia[\"eom\"] = antioquia[\"eom\"] + MonthEnd(0)\n",
    "# antioquia = antioquia.sort_values(\"eom\", ascending=False).reset_index(drop=True).set_index(\"eom\")\n",
    "# antioquia.loc[pd.to_datetime(\"2025-12-31\"), \"price\"] = 8_126\n",
    "# antioquia.loc[:, \"series_id\"] = \"Antioquia\"\n",
    "# country_avg.loc[:, \"series_id\"] = \"Country_Avg\"\n",
    "# antioquia.reset_index(inplace=True)\n",
    "# price_data = pd.concat([antioquia, country_avg], ignore_index=True, axis=0).sort_values([\"series_id\", \"eom\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "\n",
    "# raw_country_production_data = pd.read_excel(\n",
    "#     \"/Users/drest/Downloads/Book2.xlsx\",\n",
    "#     sheet_name=\"Country_production_avg\",\n",
    "# )\n",
    "\n",
    "# unpivoted_country_production_data = raw_country_production_data.melt(id_vars=[\"Mes\"], var_name=\"year\", value_name=\"value\").rename(columns={\"Mes\": \"month\"})\n",
    "# unpivoted_country_production_data[\"eom\"] = pd.to_datetime(unpivoted_country_production_data[\"year\"].astype(str) + \"-\" + unpivoted_country_production_data[\"month\"].astype(str).str.strip() + \"-01\", format=\"%Y-%b-%d\") + MonthEnd(0)\n",
    "# unpivoted_country_production_data.drop(columns=[\"year\", \"month\"], inplace=True)\n",
    "# unpivoted_country_production_data.rename(columns={\"value\": \"production\"}, inplace=True)\n",
    "# unpivoted_country_production_data.sort_values(\"eom\", ascending=False, inplace=True)\n",
    "# unpivoted_country_production_data.reset_index(drop=True, inplace=True)\n",
    "# unpivoted_country_production_data.loc[:, \"series_id\"] = \"Country_Avg\"\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Prepare data for polynomial regression\n",
    "# country_production_sorted = unpivoted_country_production_data.sort_values(\"eom\").reset_index(drop=True)\n",
    "# country_production_sorted[\"numeric_time\"] = (country_production_sorted[\"eom\"] - country_production_sorted[\"eom\"].min()).dt.days\n",
    "# X_train = country_production_sorted[\"numeric_time\"].values.reshape(-1, 1)\n",
    "# y_train = country_production_sorted[\"production\"].astype(float).values\n",
    "\n",
    "# # Remove NaN values, if any\n",
    "# mask_not_nan = ~pd.isna(y_train)\n",
    "# X_train = X_train[mask_not_nan]\n",
    "# y_train = y_train[mask_not_nan]\n",
    "\n",
    "# poly = PolynomialFeatures(degree=3)\n",
    "# X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_poly, y_train)\n",
    "\n",
    "# # Predict for 2025-12-31\n",
    "# target_date = pd.to_datetime(\"2025-12-31\")\n",
    "# target_numeric_time = (target_date - country_production_sorted[\"eom\"].min()).days\n",
    "# X_pred = poly.transform([[target_numeric_time]])\n",
    "# production_2025_12_31 = model.predict(X_pred)[0]\n",
    "\n",
    "# # Optionally, append to DataFrame (if needed for downstream use)\n",
    "# predicted_row = {\n",
    "#     \"eom\": target_date,\n",
    "#     \"production\": production_2025_12_31,\n",
    "#     \"series_id\": \"Country_Avg\"\n",
    "# }\n",
    "# unpivoted_country_production_data = pd.concat([\n",
    "#     unpivoted_country_production_data,\n",
    "#     pd.DataFrame([predicted_row])\n",
    "# ], ignore_index=True).dropna()\n",
    "\n",
    "# raw_regional_production_data = pd.read_excel(\n",
    "#     \"/Users/drest/Downloads/Book2.xlsx\",\n",
    "#     sheet_name=\"Regional_production_avg\",\n",
    "# )\n",
    "\n",
    "# regional_production_data = raw_regional_production_data.T\n",
    "# regional_production_data.columns = regional_production_data.iloc[0]\n",
    "# regional_production_data.drop(regional_production_data.index[0], inplace=True)\n",
    "# regional_production_data.reset_index(inplace=True)\n",
    "# regional_production_data.rename(columns={\"index\": \"month\"}, inplace=True)\n",
    "# regional_production_data[\"eom\"] = pd.to_datetime(regional_production_data[\"month\"].astype(str).str.strip()) + MonthEnd(0)\n",
    "# regional_production_data.drop(columns=[\"month\"], inplace=True)\n",
    "# regional_production_data = regional_production_data[[\"eom\", \"Antioquia\"]].rename(columns={\"Antioquia\": \"production\"})\n",
    "# regional_production_data.loc[:, \"series_id\"] = \"Antioquia\"\n",
    "\n",
    "# regional_production_data.columns.name = None\n",
    "# regional_production_data.index.name = None\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Prepare data for autoregression (polynomial)\n",
    "# regional_production_data_sorted = regional_production_data.sort_values(\"eom\").reset_index(drop=True)\n",
    "\n",
    "# # Use only numeric index as time to predict next value at t+1 (2025-12-31)\n",
    "# regional_production_data_sorted['numeric_time'] = (regional_production_data_sorted['eom'] - regional_production_data_sorted['eom'].min()).dt.days\n",
    "\n",
    "# # Use all but last for X_train, and corresponding for y_train\n",
    "# X_train = regional_production_data_sorted['numeric_time'].values.reshape(-1, 1)\n",
    "# y_train = regional_production_data_sorted['production'].astype(float).values\n",
    "\n",
    "# # Remove missing (NaN) production data, just in case\n",
    "# mask_not_nan = ~pd.isna(y_train)\n",
    "# X_train = X_train[mask_not_nan]\n",
    "# y_train = y_train[mask_not_nan]\n",
    "\n",
    "# # Polynomial features (degree can be changed as needed)\n",
    "# poly = PolynomialFeatures(degree=3)\n",
    "# X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_poly, y_train)\n",
    "\n",
    "# # Predict for 2025-12-31\n",
    "# target_date = pd.to_datetime(\"2025-12-31\")\n",
    "# target_numeric_time = (target_date - regional_production_data_sorted['eom'].min()).days\n",
    "# X_pred = poly.transform([[target_numeric_time]])\n",
    "# predicted_production = model.predict(X_pred)[0]\n",
    "\n",
    "# # Insert the prediction for 2025-12-31 if it's not there\n",
    "# if not (regional_production_data_sorted['eom'] == target_date).any():\n",
    "#     # Insert at the end\n",
    "#     new_row = pd.DataFrame({\n",
    "#         'eom': [target_date],\n",
    "#         'production': [predicted_production],\n",
    "#         'series_id': [\"Antioquia\"]\n",
    "#     })\n",
    "#     # Ensure columns match (add missing ones if necessary)\n",
    "#     for col in regional_production_data_sorted.columns:\n",
    "#         if col not in new_row.columns:\n",
    "#             new_row[col] = [None]\n",
    "#     regional_production_data = pd.concat([regional_production_data, new_row[regional_production_data.columns]], ignore_index=True)\n",
    "\n",
    "\n",
    "# regional_production_data.sort_values(\"eom\", ascending=False, inplace=True)\n",
    "# regional_production_data.reset_index(drop=True, inplace=True, names=None)\n",
    "# regional_production_data\n",
    "# production_data = pd.concat([unpivoted_country_production_data, regional_production_data], ignore_index=True, axis=0).sort_values([\"series_id\", \"eom\"], ascending=False).reset_index(drop=True)\n",
    "# production_data\n",
    "\n",
    "# combined = pd.merge(price_data, production_data, on=[\"eom\", \"series_id\"], how=\"inner\")\n",
    "\n",
    "# raw_consumption_data = pd.read_excel(\n",
    "#     \"/Users/drest/Downloads/Book2.xlsx\",\n",
    "#     sheet_name=\"Consumption\",\n",
    "# )\n",
    "\n",
    "# raw_consumption_data\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import numpy as np\n",
    "\n",
    "# # Prepare the data for regression\n",
    "# consumption_df = raw_consumption_data.copy()\n",
    "# X = consumption_df[['year']].values\n",
    "# y = consumption_df['per_capita_consumption'].values\n",
    "\n",
    "# # Fit the linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# # Predict per capita consumption for 2025\n",
    "# year_2025 = np.array([[2025]])\n",
    "# per_capita_2025 = model.predict(year_2025)[0]\n",
    "\n",
    "# print(f\"Predicted per_capita_consumption for 2025: {per_capita_2025:.2f}\")\n",
    "\n",
    "# raw_consumption_data.loc[len(raw_consumption_data.index)] = pd.Series({\"year\": 2025, \"per_capita_consumption\": per_capita_2025})\n",
    "# raw_consumption_data.sort_values(\"year\", ascending=True, inplace=True)\n",
    "# raw_consumption_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# # Create a monthly date range from the first year to the last year\n",
    "# monthly_dates = pd.date_range(\n",
    "#     start=f\"{raw_consumption_data['year'].min().astype(int)}-01-01\", \n",
    "#     end=f\"{raw_consumption_data['year'].max().astype(int)}-12-31\", \n",
    "#     freq='M'\n",
    "# )\n",
    "# monthly_consumption_data = pd.DataFrame({\n",
    "#     'eom': monthly_dates + MonthEnd(0),\n",
    "#     'year': monthly_dates.year,\n",
    "# })\n",
    "# raw_monthly_consumption_data = pd.merge(monthly_consumption_data, raw_consumption_data, on=\"year\", how=\"left\").drop(columns=[\"year\"]).sort_values(\"eom\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# regional_monthly_consumption_data = raw_monthly_consumption_data.copy()\n",
    "# regional_monthly_consumption_data.loc[:, \"series_id\"] = \"Antioquia\"\n",
    "# country_monthly_consumption_data = raw_monthly_consumption_data.copy()\n",
    "# country_monthly_consumption_data.loc[:, \"series_id\"] = \"Country_Avg\"\n",
    "\n",
    "# consumption_data = pd.concat([regional_monthly_consumption_data, country_monthly_consumption_data], ignore_index=True, axis=0).sort_values([\"series_id\", \"eom\"], ascending=False).reset_index(drop=True)\n",
    "# consumption_data\n",
    "\n",
    "# combined = pd.merge(combined, consumption_data, on=[\"eom\", \"series_id\"], how=\"left\")\n",
    "# combined = combined.loc[combined[\"eom\"] >= pd.to_datetime(\"2020-01-01\")]\n",
    "# combined.to_parquet(\".data/extracted/porkcolombia_combined.parquet.snappy\", index=False)\n",
    "# combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b477303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "combined = pd.read_parquet(\".data/extracted/porkcolombia_combined.parquet.snappy\")\n",
    "combined[\"year\"] = combined[\"eom\"].dt.year\n",
    "combined[\"month\"] = combined[\"eom\"].dt.month\n",
    "combined[\"time_idx\"] = combined.groupby(\"series_id\").cumcount(ascending=False)\n",
    "combined[\"price\"] = combined[\"price\"].astype(np.float64)\n",
    "combined[\"production\"] = combined[\"production\"].astype(np.float64)\n",
    "combined[\"per_capita_consumption\"] = combined[\"per_capita_consumption\"].astype(np.float64)\n",
    "combined[\"per_capita_consumption\"] = combined[\"per_capita_consumption\"] * combined[\"price\"]\n",
    "combined.sort_values([\"eom\"], ascending=True, inplace=True)\n",
    "combined.reset_index(drop=True, inplace=True)\n",
    "combined = combined.loc[combined[\"eom\"] >= pd.to_datetime(\"2020-01-01\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d347ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ts_piggies.forecast.tft import TFTConfig, TFTForecastWrapper\n",
    "# from ts_piggies.helpers.scaler import MinMaxScalerWrapper\n",
    "\n",
    "# minmax_scaler = MinMaxScalerWrapper()\n",
    "# combined_scaled = minmax_scaler.fit_transform(combined, [\"price\", \"production\", \"per_capita_consumption\"])\n",
    "\n",
    "# wrapper = TFTForecastWrapper(\n",
    "#     combined_scaled,\n",
    "#     TFTConfig(\n",
    "#         learning_rate=0.003,\n",
    "#         hidden_size=64,\n",
    "#         attention_head_size=8,\n",
    "#         dropout=0.01,\n",
    "#         hidden_continuous_size=64,\n",
    "#         batch_size=32,\n",
    "#         max_epochs=100,\n",
    "#         gradient_clip_val=0.01,\n",
    "#         reduce_on_plateau_patience=10,\n",
    "#     ),\n",
    "#     max_encoder_length=64,\n",
    "#     max_prediction_length=7,\n",
    "# )\n",
    "\n",
    "# result = wrapper.fit_predict(simulations=True)\n",
    "\n",
    "# results = pd.DataFrame({\"eom\": result.dates, \"price\": result.mean})\n",
    "\n",
    "# results = minmax_scaler.inverse_transform(results, [\"price\"])\n",
    "\n",
    "# correction = combined.iloc[-1][\"price\"] - results.iloc[0][\"price\"]\n",
    "# results[\"price\"] = results[\"price\"].astype(np.float64) + correction\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2508cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from ts_piggies.helpers.scaler import MinMaxScalerWrapper\n",
    "from ts_piggies.forecast.grid import ErrorMetric, GridSearch, GridSearchConfig\n",
    "from ts_piggies.forecast.tft import TFTConfig, TFTForecastWrapper\n",
    "\n",
    "logger = logging.getLogger(\"ts_piggies\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "max_encoder_length = 64\n",
    "max_prediction_length = 12\n",
    "\n",
    "minmax_scaler = MinMaxScalerWrapper()\n",
    "combined_scaled = minmax_scaler.fit_transform(combined, [\"price\", \"production\", \"per_capita_consumption\"])\n",
    "\n",
    "custom_grid = {\n",
    "    \"learning_rate\": [0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.04],  # 7 values - expanded range\n",
    "    \"hidden_size\": [128, 256, 512],  # 4 values - includes smaller and larger models\n",
    "    \"attention_head_size\": [4, 6, 8],  # 4 values - varied attention mechanisms\n",
    "    \"dropout\": [0.1, 0.15, 0.2, 0.25],  # 5 values - regularization range\n",
    "    \"hidden_continuous_size\": [16, 32, 48, 64],  # 4 values - continuous feature processing\n",
    "    \"batch_size\": [32, 64, 128],  # 3 values - different batch sizes\n",
    "    \"max_epochs\": [100, 120, 150, 250, 300],  # 5 values - training duration variation\n",
    "    \"gradient_clip_val\": [0.05, 0.1, 0.15, 0.2],  # 4 values - gradient clipping\n",
    "    \"reduce_on_plateau_patience\": [4, 6, 8],  # 3 values - learning rate scheduling\n",
    "}\n",
    "\n",
    "grid_config = GridSearchConfig(\n",
    "    config_type=TFTConfig,\n",
    "    model_type=TFTForecastWrapper,\n",
    "    hyperparameter_grid=custom_grid,\n",
    "    segment_columns=[\"series_id\"],\n",
    "    sort_columns=[\"eom\"],\n",
    "    test_column=\"price\",\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    n_simulations=1000,\n",
    "    error_metric=ErrorMetric.MAPE,\n",
    "    validation_length=0.1,\n",
    ")\n",
    "\n",
    "# Initialize grid search\n",
    "grid_search = GridSearch(\n",
    "    data=combined_scaled,\n",
    "    config=grid_config,\n",
    ")\n",
    "\n",
    "# Run grid search with at least 40 models for statistically convincing results\n",
    "# Using random_sample=True ensures diverse hyperparameter combinations\n",
    "# This provides better coverage of the hyperparameter space\n",
    "forecast_results = grid_search.search(max_models=250, random_sample=True).fit_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inverse transform the probabilistic forecasts back to original scale\n",
    "\n",
    "# Create a DataFrame with the forecast values for inverse transformation\n",
    "from ts_piggies.forecast.abstract import ForecastResult, ProbabilisticForecastResult\n",
    "\n",
    "\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"price\": forecast_results.mean\n",
    "})\n",
    "\n",
    "# Inverse transform the mean forecast\n",
    "forecast_df_inverse = minmax_scaler.inverse_transform(forecast_df, columns=[\"price\"])\n",
    "mean_inverse = forecast_df_inverse[\"price\"].tolist()\n",
    "\n",
    "forecast_df_inverse = pd.DataFrame({\n",
    "    \"price\": forecast_results.median\n",
    "})\n",
    "\n",
    "# Inverse transform the median forecast\n",
    "forecast_df_inverse = minmax_scaler.inverse_transform(forecast_df, columns=[\"price\"])\n",
    "median_inverse = forecast_df_inverse[\"price\"].tolist()\n",
    "\n",
    "\n",
    "nearer_to_zero_mean = (combined.iloc[-1][\"price\"] - mean_inverse[0])\n",
    "nearer_to_zero_median = (combined.iloc[-1][\"price\"] - median_inverse[0])\n",
    "\n",
    "correction = nearer_to_zero_mean if abs(nearer_to_zero_mean) < abs(nearer_to_zero_median) else nearer_to_zero_median\n",
    "\n",
    "mean_inverse = [x + correction for x in mean_inverse]\n",
    "median_inverse = [x + correction for x in median_inverse]\n",
    "\n",
    "# Inverse transform the std (standard deviation scales with the same transformation)\n",
    "# Note: std should be scaled by the same factor as the mean\n",
    "std_df = pd.DataFrame({\n",
    "    \"price\": forecast_results.std\n",
    "})\n",
    "std_inverse = minmax_scaler.inverse_transform(std_df, columns=[\"price\"]).values.flatten().tolist()\n",
    "# std_inverse = forecast_df_inverse[\"price\"].values * np.array(forecast_results.std)\n",
    "\n",
    "# Inverse transform all quantiles\n",
    "quantiles_inverse = {}\n",
    "for quantile_name, quantile_values in forecast_results.quantiles.items():\n",
    "    quantile_df = pd.DataFrame({\n",
    "        \"price\": quantile_values\n",
    "    })\n",
    "    quantile_df_inverse = minmax_scaler.inverse_transform(quantile_df, columns=[\"price\"])\n",
    "    quantiles_inverse[quantile_name] = (quantile_df_inverse[\"price\"] + correction).tolist()\n",
    "\n",
    "# Inverse transform individual forecasts\n",
    "individual_forecasts_inverse = []\n",
    "for forecast_result in forecast_results.individual_forecasts:\n",
    "    individual_df = pd.DataFrame({\n",
    "        \"price\": forecast_result.forecast\n",
    "    })\n",
    "    individual_df_inverse = minmax_scaler.inverse_transform(individual_df, columns=[\"price\"])\n",
    "    \n",
    "    # Create new ForecastResult with inverse transformed values\n",
    "    forecast_result_inverse = ForecastResult(\n",
    "        model_name=forecast_result.model_name,\n",
    "        config=forecast_result.config,\n",
    "        forecast=(individual_df_inverse[\"price\"] + correction).tolist(),\n",
    "        dates=forecast_result.dates,\n",
    "        train_loss=forecast_result.train_loss,\n",
    "    )\n",
    "    individual_forecasts_inverse.append(forecast_result_inverse)\n",
    "\n",
    "\n",
    "# Create new ProbabilisticForecastResult with inverse transformed values\n",
    "probabilistic_forecast_inverse = ProbabilisticForecastResult(\n",
    "    dates=forecast_results.dates,\n",
    "    mean=mean_inverse,\n",
    "    median=median_inverse,\n",
    "    std=std_inverse,\n",
    "    quantiles=quantiles_inverse,\n",
    "    individual_forecasts=individual_forecasts_inverse,\n",
    "    n_models=forecast_results.n_models,\n",
    ")\n",
    "\n",
    "# Display inverse transformed results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Inverse Transformed Probabilistic Forecast Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Number of models: {probabilistic_forecast_inverse.n_models}\")\n",
    "print(f\"Forecast period: {len(probabilistic_forecast_inverse.mean)} months\")\n",
    "print(f\"\\nMean forecast (first 3 months) - Original Scale:\")\n",
    "for i, (date, mean_val) in enumerate(zip(probabilistic_forecast_inverse.dates[:3], probabilistic_forecast_inverse.mean[:3])):\n",
    "    std_val = probabilistic_forecast_inverse.std[i]\n",
    "    print(f\"  {date}: {mean_val:.2f} ± {std_val:.2f}\")\n",
    "\n",
    "# Plot inverse transformed forecast with historical data (original scale)\n",
    "probabilistic_forecast_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.DataFrame(\n",
    "    data={\n",
    "        \"eom\": probabilistic_forecast_inverse.dates,\n",
    "        \"mean\": probabilistic_forecast_inverse.mean,\n",
    "        \"median\": probabilistic_forecast_inverse.median,\n",
    "        \"std\": probabilistic_forecast_inverse.std,\n",
    "        **{ f\"quantile_{int(float(col) * 100)}\": li for col, li in probabilistic_forecast_inverse.quantiles.items()},\n",
    "    },\n",
    ")\n",
    "\n",
    "forecast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c89cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram comparing forecast distribution vs historical price distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Collect all forecast values from all individual models (already inverse transformed)\n",
    "all_forecast_values = []\n",
    "for forecast_result in probabilistic_forecast_inverse.individual_forecasts:\n",
    "    all_forecast_values.extend(forecast_result.forecast)\n",
    "\n",
    "# Get historical price data (original scale, not scaled)\n",
    "historical_prices = combined[\"price\"].dropna().values\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left plot: Overlaid histograms\n",
    "ax1 = axes[0]\n",
    "ax1.hist(\n",
    "    historical_prices, \n",
    "    bins=30, \n",
    "    alpha=0.6, \n",
    "    label=f'Historical Prices (n={len(historical_prices)})', \n",
    "    color='steelblue',\n",
    "    density=True,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "ax1.hist(\n",
    "    all_forecast_values, \n",
    "    bins=30, \n",
    "    alpha=0.6, \n",
    "    label=f'All Forecasts (n={len(all_forecast_values)})', \n",
    "    color='coral',\n",
    "    density=True,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Add vertical lines for means\n",
    "hist_mean = np.mean(historical_prices)\n",
    "forecast_mean = np.mean(all_forecast_values)\n",
    "ax1.axvline(hist_mean, color='steelblue', linestyle='--', linewidth=2, label=f'Historical Mean: {hist_mean:.2f}')\n",
    "ax1.axvline(forecast_mean, color='coral', linestyle='--', linewidth=2, label=f'Forecast Mean: {forecast_mean:.2f}')\n",
    "\n",
    "ax1.set_xlabel('Price (Original Scale)', fontsize=12)\n",
    "ax1.set_ylabel('Density', fontsize=12)\n",
    "ax1.set_title('Distribution Comparison: Historical vs Forecasts', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Side-by-side box plots\n",
    "ax2 = axes[1]\n",
    "box_data = [historical_prices, all_forecast_values]\n",
    "box_labels = [f'Historical\\n(n={len(historical_prices)})', f'All Forecasts\\n(n={len(all_forecast_values)})']\n",
    "bp = ax2.boxplot(box_data, labels=box_labels, patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color the boxes\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor('coral')\n",
    "bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "# Color the medians\n",
    "for median in bp['medians']:\n",
    "    median.set_color('black')\n",
    "    median.set_linewidth(2)\n",
    "\n",
    "ax2.set_ylabel('Price (Original Scale)', fontsize=12)\n",
    "ax2.set_title('Box Plot Comparison: Historical vs Forecasts', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"\"\"\n",
    "Statistics Comparison:\n",
    "\n",
    "Historical Prices:\n",
    "  Mean: {np.mean(historical_prices):.2f}\n",
    "  Median: {np.median(historical_prices):.2f}\n",
    "  Std: {np.std(historical_prices):.2f}\n",
    "  Min: {np.min(historical_prices):.2f}\n",
    "  Max: {np.max(historical_prices):.2f}\n",
    "\n",
    "All Forecasts:\n",
    "  Mean: {np.mean(all_forecast_values):.2f}\n",
    "  Median: {np.median(all_forecast_values):.2f}\n",
    "  Std: {np.std(all_forecast_values):.2f}\n",
    "  Min: {np.min(all_forecast_values):.2f}\n",
    "  Max: {np.max(all_forecast_values):.2f}\n",
    "\"\"\"\n",
    "\n",
    "print(stats_text)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional: Create a single combined histogram with better styling\n",
    "fig2, ax3 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create histogram with better binning\n",
    "n_bins = max(30, int(np.sqrt(len(historical_prices)) + np.sqrt(len(all_forecast_values))))\n",
    "bins = np.linspace(\n",
    "    min(np.min(historical_prices), np.min(all_forecast_values)),\n",
    "    max(np.max(historical_prices), np.max(all_forecast_values)),\n",
    "    n_bins\n",
    ")\n",
    "\n",
    "ax3.hist(\n",
    "    historical_prices, \n",
    "    bins=bins, \n",
    "    alpha=0.5, \n",
    "    label=f'Historical Prices (n={len(historical_prices)})', \n",
    "    color='steelblue',\n",
    "    density=True,\n",
    "    edgecolor='darkblue',\n",
    "    linewidth=1.5\n",
    ")\n",
    "ax3.hist(\n",
    "    all_forecast_values, \n",
    "    bins=bins, \n",
    "    alpha=0.5, \n",
    "    label=f'All Forecasts (n={len(all_forecast_values)})', \n",
    "    color='coral',\n",
    "    density=True,\n",
    "    edgecolor='darkred',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add mean lines\n",
    "ax3.axvline(hist_mean, color='steelblue', linestyle='--', linewidth=2.5, \n",
    "            label=f'Historical Mean: {hist_mean:.2f}', alpha=0.8)\n",
    "ax3.axvline(forecast_mean, color='coral', linestyle='--', linewidth=2.5, \n",
    "            label=f'Forecast Mean: {forecast_mean:.2f}', alpha=0.8)\n",
    "\n",
    "# Add median lines\n",
    "hist_median = np.median(historical_prices)\n",
    "forecast_median = np.median(all_forecast_values)\n",
    "ax3.axvline(hist_median, color='steelblue', linestyle=':', linewidth=2, \n",
    "            label=f'Historical Median: {hist_median:.2f}', alpha=0.6)\n",
    "ax3.axvline(forecast_median, color='coral', linestyle=':', linewidth=2, \n",
    "            label=f'Forecast Median: {forecast_median:.2f}', alpha=0.6)\n",
    "\n",
    "ax3.set_xlabel('Price (Original Scale)', fontsize=13, fontweight='bold')\n",
    "ax3.set_ylabel('Density', fontsize=13, fontweight='bold')\n",
    "ax3.set_title('Price Distribution: Historical Data vs Ensemble Forecasts', fontsize=15, fontweight='bold')\n",
    "ax3.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e522c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive line plot with historical data, forecast quantiles, mean, median, std, and trendline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Prepare historical data\n",
    "historical_df = combined[[\"eom\", \"price\", \"series_id\"]].copy()\n",
    "historical_df[\"eom\"] = pd.to_datetime(historical_df[\"eom\"])\n",
    "\n",
    "std_multiplier = .5\n",
    "# Prepare forecast data\n",
    "forecast_dates = pd.to_datetime(probabilistic_forecast_inverse.dates)\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"eom\": forecast_dates,\n",
    "    \"mean\": probabilistic_forecast_inverse.mean,\n",
    "    \"median\": probabilistic_forecast_inverse.median,\n",
    "    \"std\": probabilistic_forecast_inverse.std,\n",
    "    \"quantile_10\": probabilistic_forecast_inverse.quantiles.get(\"0.1\", []),\n",
    "    \"quantile_25\": probabilistic_forecast_inverse.quantiles.get(\"0.25\", []),\n",
    "    \"quantile_50\": probabilistic_forecast_inverse.quantiles.get(\"0.5\", []),\n",
    "    \"quantile_75\": probabilistic_forecast_inverse.quantiles.get(\"0.75\", []),\n",
    "    \"quantile_90\": probabilistic_forecast_inverse.quantiles.get(\"0.9\", []),\n",
    "})\n",
    "\n",
    "# Calculate upper and lower bounds for std\n",
    "forecast_df[\"mean_plus_std\"] = forecast_df[\"mean\"] + (forecast_df[\"std\"] * std_multiplier)\n",
    "forecast_df[\"mean_minus_std\"] = forecast_df[\"mean\"] - (forecast_df[\"std\"] * std_multiplier)\n",
    "\n",
    "# Get unique series\n",
    "unique_series = historical_df[\"series_id\"].unique()\n",
    "n_series = len(unique_series)\n",
    "\n",
    "# Create subplots - one for each series\n",
    "if n_series > 1:\n",
    "    fig, axes = plt.subplots(n_series, 1, figsize=(16, 6 * n_series), sharex=True)\n",
    "    # Ensure axes is always a list/array we can index\n",
    "    if not isinstance(axes, np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    axes = [ax]\n",
    "\n",
    "for idx, series_id in enumerate(unique_series):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Filter historical data for this series\n",
    "    series_historical = historical_df[historical_df[\"series_id\"] == series_id].copy()\n",
    "    series_historical = series_historical.sort_values(\"eom\")\n",
    "    last_historical_date = series_historical[\"eom\"].max()\n",
    "\n",
    "    # Calculate trendline using only forecast mean\n",
    "    # Convert forecast dates to numeric for regression\n",
    "    forecast_date_numeric = (forecast_df[\"eom\"] - forecast_df[\"eom\"].min()).dt.days.values\n",
    "    trend_slope, trend_intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        forecast_date_numeric, forecast_df[\"mean\"]\n",
    "    )\n",
    "    trendline = trend_slope * forecast_date_numeric + trend_intercept\n",
    "    \n",
    "    # Plot historical data\n",
    "    sns.lineplot(\n",
    "        data=series_historical,\n",
    "        x=\"eom\",\n",
    "        y=\"price\",\n",
    "        ax=ax,\n",
    "        label=\"Historical Price\",\n",
    "        color=\"black\",\n",
    "        linewidth=2,\n",
    "        marker=\"o\",\n",
    "        markersize=4,\n",
    "        alpha=0.8\n",
    "    )\n",
    "    \n",
    "    # Connect historical price to first forecast mean value\n",
    "    last_historical_date = series_historical[\"eom\"].max()\n",
    "    last_historical_price = series_historical[\"price\"].iloc[-1]\n",
    "    first_forecast_date = forecast_df[\"eom\"].iloc[0]\n",
    "    first_forecast_mean = forecast_df[\"mean\"].iloc[0]\n",
    "    \n",
    "    # Draw connecting line\n",
    "    ax.plot(\n",
    "        [last_historical_date, first_forecast_date],\n",
    "        [last_historical_price, first_forecast_mean],\n",
    "        color=\"black\",\n",
    "        linewidth=2,\n",
    "        alpha=0.8,\n",
    "        linestyle=\"-\"\n",
    "    )\n",
    "    \n",
    "    # Plot forecast quantile regions (shaded areas)\n",
    "    # 10th-90th percentile region\n",
    "    ax.fill_between(\n",
    "        forecast_df[\"eom\"],\n",
    "        forecast_df[\"quantile_10\"],\n",
    "        forecast_df[\"quantile_90\"],\n",
    "        alpha=0.2,\n",
    "        color=\"blue\",\n",
    "        label=\"10th-90th Percentile\"\n",
    "    )\n",
    "    \n",
    "    # 25th-75th percentile region\n",
    "    ax.fill_between(\n",
    "        forecast_df[\"eom\"],\n",
    "        forecast_df[\"quantile_25\"],\n",
    "        forecast_df[\"quantile_75\"],\n",
    "        alpha=0.3,\n",
    "        color=\"blue\",\n",
    "        label=\"25th-75th Percentile\"\n",
    "    )\n",
    "        \n",
    "    ax.fill_between(\n",
    "        forecast_df[\"eom\"],\n",
    "        forecast_df[\"mean_minus_std\"],\n",
    "        forecast_df[\"mean_plus_std\"],\n",
    "        alpha=0.15,\n",
    "        color=\"orange\",\n",
    "        label=f\"Mean ± 1 Std\"\n",
    "    )\n",
    "    \n",
    "    # Plot quantile lines\n",
    "    sns.lineplot(\n",
    "        x=forecast_df[\"eom\"],\n",
    "        y=forecast_df[\"quantile_10\"],\n",
    "        ax=ax,\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=\"10th Percentile\"\n",
    "    )\n",
    "    sns.lineplot(\n",
    "        x=forecast_df[\"eom\"],\n",
    "        y=forecast_df[\"quantile_90\"],\n",
    "        ax=ax,\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1,\n",
    "        alpha=0.6,\n",
    "        label=\"90th Percentile\"\n",
    "    )\n",
    "    \n",
    "    # Plot mean\n",
    "    sns.lineplot(\n",
    "        x=forecast_df[\"eom\"],\n",
    "        y=forecast_df[\"mean\"],\n",
    "        ax=ax,\n",
    "        color=\"red\",\n",
    "        linewidth=2.5,\n",
    "        linestyle=\"-\",\n",
    "        label=\"Forecast Mean\"\n",
    "    )\n",
    "    \n",
    "    # Plot median\n",
    "    sns.lineplot(\n",
    "        x=forecast_df[\"eom\"],\n",
    "        y=forecast_df[\"median\"],\n",
    "        ax=ax,\n",
    "        color=\"green\",\n",
    "        linewidth=2.5,\n",
    "        linestyle=\"-\",\n",
    "        label=\"Forecast Median\"\n",
    "    )\n",
    "    \n",
    "    # Plot trendline (only for forecast period)\n",
    "    sns.lineplot(\n",
    "        x=forecast_df[\"eom\"],\n",
    "        y=trendline,\n",
    "        ax=ax,\n",
    "        color=\"purple\",\n",
    "        linewidth=2,\n",
    "        linestyle=\":\",\n",
    "        label=f\"Trendline (R²={r_value**2:.3f})\"\n",
    "    )\n",
    "    \n",
    "    # Add vertical line to separate historical and forecast\n",
    "    ax.axvline(\n",
    "        x=last_historical_date,\n",
    "        color=\"gray\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        alpha=0.7,\n",
    "        label=\"Forecast Start\"\n",
    "    )\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel(\"Date (End of Month)\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Price\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        f\"Price Forecast with Uncertainty Intervals - {series_id}\\n\"\n",
    "        f\"Mean: {forecast_df['mean'].mean():.2f} ± {forecast_df['std'].mean():.2f}, \"\n",
    "        f\"Trend: {trend_slope*30:.2f}/month\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\"\n",
    "    )\n",
    "    ax.legend(loc=\"best\", fontsize=10, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "    \n",
    "    # Format y-axis to show currency\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f\"${x:,.0f}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Forecast Summary Statistics\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of series: {n_series}\")\n",
    "print(f\"Forecast period: {len(forecast_df)} months\")\n",
    "print(f\"\\nForecast Statistics (across all dates):\")\n",
    "print(f\"  Mean: {forecast_df['mean'].mean():.2f} ± {forecast_df['std'].mean():.2f}\")\n",
    "print(f\"  Median: {forecast_df['median'].mean():.2f}\")\n",
    "print(f\"  Min (10th percentile): {forecast_df['quantile_10'].min():.2f}\")\n",
    "print(f\"  Max (90th percentile): {forecast_df['quantile_90'].max():.2f}\")\n",
    "print(f\"  Trend slope: {trend_slope*30:.2f} per month\")\n",
    "print(f\"  Trend R²: {r_value**2:.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-experiments-dYK2w0Zu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
